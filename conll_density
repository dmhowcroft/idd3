#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Customized version of IDD3's run.py
# Copyright (C) 2015  David M. Howcroft
#
# This program is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License as published by the Free
# Software Foundation, either version 3 of the License, or (at your option)
# any later version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
# more details.
#
# You should have received a copy of the GNU General Public License along with
# this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import print_function, unicode_literals, division

from idd3 import Relation, Engine, rules, transform
import nltk
from sys import argv, stdout
from subprocess import call
from collections import defaultdict

import logging
from traceback import print_exc

logging.basicConfig(level=logging.INFO)

import os
# columns is the width of the terminal the user is viewing
# This is used for output pretty printing
try:
    _, columns = os.popen('stty size', 'r').read().split()
except ValueError:
    columns = 72

try:
    from termcolor import colored
    raise ImportError
except ImportError:
    def colored(string, color, attrs):
        return string

# We write the Stanford parse to a file as an intermediate stage in our processing.
TEMPORARY_FILENAME = 'tmp.tree'


# Stanford parser
# Change this variable to the path on your system
stanford_path = os.path.expanduser('~') + "/apps/stanford-corenlp"
# Define the path to the directory containing the version of Java you want to use
java_dir = '/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/'


STANFORD_PARSER_INVOCATION = java_dir + 'java -mx1024m -cp ' + stanford_path + \
    '/*: edu.stanford.nlp.parser.lexparser.LexicalizedParser ' + \
    '-maxLength 160 -outputFormat conll2007 -outputFormatOptions includePunctuationDependencies ' \
    '-sentences newline -originalDependencies edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz {}'


def get_sentence(graph):
    """Turns a graph into a list of words.
    """
    return ' '.join([str(graph.nodes[node]['word']) for node in graph.nodes if 'word' in graph.nodes[node]])


def process_graphs(graphs, verbose=False):
    engine = Engine(rules.all_rulesets, transform.all_transformations)
    stats = defaultdict(int)

    for sent_num, graph in enumerate(graphs):
        if verbose:
            print('-' * int(columns))

        relations = []
        for node in graph.nodes:
            relation = graph.nodes[node]
            try:
                if relation['rel'] == 'ROOT':
                    relation['rel'] = 'root'
                if 'ROOT' in relation['deps']:
                    relation['deps']['root'] = relation['deps'].pop('ROOT')
                relations.append(Relation(**relation))
            except KeyError:
                # TODO check that this is okay
                pass

        if verbose:
            print(colored('Sentence %d:' % (sent_num, ), 'white', attrs=['bold']))
            print('\t' + get_sentence(graph))

            # print('Relations:')
            # print(relations)

            print(colored('Propositions:', 'white', attrs=['bold']))

        try:
            engine.analyze(relations)
            for prop_num, prop in enumerate(engine.props):
                # if verbose:
                #     print(str(prop_num + 1) + ' ' + str(prop))
                stats[prop.kind] += 1
        except IndexError:
            print("Failed to process:")
            if verbose:
                print(relations)
                print_exc(file=stdout)
            print("------------------")
            stats = {'P': -1, 'M': -1, 'C': -1}
        except TypeError:
            print("Failed to process:")
            print(" ".join([relations[i].word for i in range(1, len(relations))]))
            if verbose:
                print(relations)
                print_exc()
            print("------------------")
            stats = {'P': -2, 'M': -1, 'C': -1}

    if verbose:
        print('-' * int(columns))
    return stats


def print_stats(stats):
    print('Stats:')
    print('Kind\t#\t')
    for kind, n in stats.items():
        print('{0}\t{1}'.format(kind, n))


def get_sentfeats(stats):
    total = 0
    vals = []
    for kind in ['P', 'M', 'C']:
        n = stats.get(kind, 0)
        total += n
        vals.append(str(n))
    return str(total) + " " + " ".join(vals) + "\n"


def print_usage():
        print('Usage: python', argv[0], ' <mode> <input file> <output file>')
        print()
        print('  <mode>           conll (to generate a CONLL file from a SENTS file; or')
        print('                   density (to generate idea density features from a CONLL file)')
        print('  <input file>     read as input')
        print('  <output file>    will be overwritten (in conll mode)')


def main():
    if len(argv) < 2 or "-h" in argv:
        print_usage()
        exit()

    in_filename = argv[2]
    if len(argv) > 3:
        out_filename = argv[3]
    else:
        out_filename = in_filename+".idd3."+argv[1]

    if argv[1] == 'density':
        graphs = nltk.parse.dependencygraph.DependencyGraph.load(in_filename)

        out_file = open(out_filename, 'w')
        # the last graph is empty, cruft from the end of the file
        graphs = graphs[:-1]
        for graph in graphs:
            # print(graph)
            stats = process_graphs([graph])
            # print(stats)

            out_file.write(get_sentfeats(stats))
        out_file.close()

    elif argv[1] == 'conll':
        conll_file = open(out_filename, 'w')
        conll_err = open(out_filename+'.stderr', 'w')

        call(STANFORD_PARSER_INVOCATION.format(in_filename).split(' '), stdout=conll_file, stderr=conll_err)

        conll_file.close()
        conll_err.close()

        # Rewrite the root node label to match NLTK's expectations
        conll_file = open(out_filename, 'r')
        conll_data = conll_file.read()
        conll_file.close()
        conll_file = open(out_filename, 'w')
        conll_data = conll_data.replace("root", "ROOT")
        conll_file.write(conll_data)
        conll_file.close()
    else:
        print_usage()
        exit()


if __name__ == '__main__':
    main()
